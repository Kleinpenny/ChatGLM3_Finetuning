{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a1de889-b29b-4146-98f2-6fc5cc5c6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Package                        Version\n",
      "------------------------------ ---------------\n",
      "absl-py                        2.0.0\n",
      "accelerate                     0.28.0\n",
      "addict                         2.4.0\n",
      "aiohttp                        3.9.3\n",
      "aiosignal                      1.3.1\n",
      "aliyun-python-sdk-core         2.15.0\n",
      "aliyun-python-sdk-kms          2.16.2\n",
      "anyio                          4.2.0\n",
      "argon2-cffi                    23.1.0\n",
      "argon2-cffi-bindings           21.2.0\n",
      "arrow                          1.3.0\n",
      "asttokens                      2.4.1\n",
      "async-lru                      2.0.4\n",
      "async-timeout                  4.0.3\n",
      "attrs                          23.2.0\n",
      "Babel                          2.14.0\n",
      "beautifulsoup4                 4.12.2\n",
      "bitsandbytes                   0.39.0\n",
      "bleach                         6.1.0\n",
      "brotlipy                       0.7.0\n",
      "cachetools                     5.3.2\n",
      "certifi                        2022.12.7\n",
      "cffi                           1.15.1\n",
      "charset-normalizer             2.0.4\n",
      "comm                           0.2.1\n",
      "conda                          22.11.1\n",
      "conda-content-trust            0.1.3\n",
      "conda-package-handling         1.9.0\n",
      "contourpy                      1.2.0\n",
      "crcmod                         1.7\n",
      "cryptography                   38.0.1\n",
      "cycler                         0.12.1\n",
      "datasets                       2.18.0\n",
      "debugpy                        1.8.0\n",
      "decorator                      5.1.1\n",
      "defusedxml                     0.7.1\n",
      "dill                           0.3.8\n",
      "einops                         0.7.0\n",
      "exceptiongroup                 1.2.0\n",
      "executing                      2.0.1\n",
      "fastjsonschema                 2.19.1\n",
      "filelock                       3.13.1\n",
      "fonttools                      4.47.0\n",
      "fqdn                           1.5.1\n",
      "frozenlist                     1.4.1\n",
      "fsspec                         2023.12.2\n",
      "gast                           0.5.4\n",
      "google-auth                    2.26.1\n",
      "google-auth-oauthlib           1.2.0\n",
      "grpcio                         1.60.0\n",
      "h11                            0.14.0\n",
      "httpcore                       1.0.4\n",
      "httpx                          0.27.0\n",
      "huggingface-hub                0.21.4\n",
      "idna                           3.4\n",
      "importlib_metadata             7.0.2\n",
      "inquirerpy                     0.3.4\n",
      "ipykernel                      6.28.0\n",
      "ipython                        8.20.0\n",
      "ipywidgets                     8.1.1\n",
      "isoduration                    20.11.0\n",
      "jedi                           0.19.1\n",
      "Jinja2                         3.1.2\n",
      "jmespath                       0.10.0\n",
      "json5                          0.9.14\n",
      "jsonpointer                    2.4\n",
      "jsonschema                     4.20.0\n",
      "jsonschema-specifications      2023.12.1\n",
      "jupyter                        1.0.0\n",
      "jupyter_client                 8.6.0\n",
      "jupyter-console                6.6.3\n",
      "jupyter_core                   5.7.1\n",
      "jupyter-events                 0.9.0\n",
      "jupyter-lsp                    2.2.1\n",
      "jupyter_server                 2.12.2\n",
      "jupyter_server_terminals       0.5.1\n",
      "jupyterlab                     4.1.5\n",
      "jupyterlab-language-pack-zh-CN 4.0.post6\n",
      "jupyterlab_pygments            0.3.0\n",
      "jupyterlab_server              2.25.2\n",
      "jupyterlab-widgets             3.0.9\n",
      "kiwisolver                     1.4.5\n",
      "Markdown                       3.5.1\n",
      "MarkupSafe                     2.1.3\n",
      "matplotlib                     3.8.2\n",
      "matplotlib-inline              0.1.6\n",
      "mistune                        3.0.2\n",
      "modelscope                     1.13.1\n",
      "mpmath                         1.3.0\n",
      "multidict                      6.0.5\n",
      "multiprocess                   0.70.16\n",
      "nbclient                       0.9.0\n",
      "nbconvert                      7.14.0\n",
      "nbformat                       5.9.2\n",
      "nest-asyncio                   1.5.8\n",
      "networkx                       3.2.1\n",
      "notebook                       7.1.2\n",
      "notebook_shim                  0.2.3\n",
      "numpy                          1.26.3\n",
      "oauthlib                       3.2.2\n",
      "oss2                           2.18.4\n",
      "overrides                      7.4.0\n",
      "packaging                      23.2\n",
      "pandas                         2.2.1\n",
      "pandocfilters                  1.5.0\n",
      "parso                          0.8.3\n",
      "peft                           0.9.0\n",
      "pexpect                        4.9.0\n",
      "pfzy                           0.3.4\n",
      "pillow                         10.2.0\n",
      "pip                            22.3.1\n",
      "platformdirs                   4.1.0\n",
      "pluggy                         1.0.0\n",
      "prometheus-client              0.19.0\n",
      "prompt-toolkit                 3.0.43\n",
      "protobuf                       4.23.4\n",
      "psutil                         5.9.7\n",
      "ptyprocess                     0.7.0\n",
      "pure-eval                      0.2.2\n",
      "pyarrow                        15.0.1\n",
      "pyarrow-hotfix                 0.6\n",
      "pyasn1                         0.5.1\n",
      "pyasn1-modules                 0.3.0\n",
      "pycosat                        0.6.4\n",
      "pycparser                      2.21\n",
      "pycryptodome                   3.20.0\n",
      "Pygments                       2.17.2\n",
      "pyOpenSSL                      22.0.0\n",
      "pyparsing                      3.1.1\n",
      "PySocks                        1.7.1\n",
      "python-dateutil                2.8.2\n",
      "python-json-logger             2.0.7\n",
      "pytz                           2024.1\n",
      "PyYAML                         6.0.1\n",
      "pyzmq                          25.1.2\n",
      "qtconsole                      5.5.1\n",
      "QtPy                           2.4.1\n",
      "referencing                    0.32.1\n",
      "regex                          2023.12.25\n",
      "requests                       2.31.0\n",
      "requests-oauthlib              1.3.1\n",
      "rfc3339-validator              0.1.4\n",
      "rfc3986-validator              0.1.1\n",
      "rpds-py                        0.16.2\n",
      "rsa                            4.9\n",
      "ruamel.yaml                    0.17.21\n",
      "ruamel.yaml.clib               0.2.6\n",
      "safetensors                    0.4.2\n",
      "scipy                          1.12.0\n",
      "Send2Trash                     1.8.2\n",
      "sentencepiece                  0.2.0\n",
      "setuptools                     65.5.0\n",
      "simplejson                     3.19.2\n",
      "six                            1.16.0\n",
      "sniffio                        1.3.0\n",
      "sortedcontainers               2.4.0\n",
      "soupsieve                      2.5\n",
      "stack-data                     0.6.3\n",
      "supervisor                     4.2.5\n",
      "sympy                          1.12\n",
      "tensorboard                    2.15.1\n",
      "tensorboard-data-server        0.7.2\n",
      "terminado                      0.18.0\n",
      "tinycss2                       1.2.1\n",
      "tokenizers                     0.15.2\n",
      "tomli                          2.0.1\n",
      "toolz                          0.12.0\n",
      "torch                          2.1.2+cu121\n",
      "torchvision                    0.16.2+cu121\n",
      "tornado                        6.4\n",
      "tqdm                           4.64.1\n",
      "traitlets                      5.14.1\n",
      "transformers                   4.36.0\n",
      "triton                         2.1.0\n",
      "types-python-dateutil          2.8.19.20240106\n",
      "typing_extensions              4.9.0\n",
      "tzdata                         2024.1\n",
      "uri-template                   1.3.0\n",
      "urllib3                        1.26.13\n",
      "wcwidth                        0.2.13\n",
      "webcolors                      1.13\n",
      "webencodings                   0.5.1\n",
      "websocket-client               1.7.0\n",
      "Werkzeug                       3.0.1\n",
      "wheel                          0.37.1\n",
      "widgetsnbextension             4.0.9\n",
      "xxhash                         3.4.1\n",
      "yapf                           0.40.2\n",
      "yarl                           1.9.4\n",
      "zipp                           3.18.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b87d2-c4fd-404f-8e07-f67c9f6a5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.30.0 datasets peft accelerate==0.28.0 bitsandbytes==0.39.0 safetensors\n",
    "# python 3.8 + torch 2.0.0 + cuda 11.8\n",
    "#或者 python 3.10 + torch 2.1.0 + cuda 12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c4c279-d3cf-4054-9feb-3c794d14aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b86f80-0d08-4110-a553-a8ebdbef1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /root/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//u343413-ab6b-40f3a764.neimeng.seetacloud.com'), PosixPath('6443')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da46843b54d54e74ba17c531d22ad88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 404280320. All model parameters: 6748165120 \n",
      "trainable model parameters: 6553600. All model parameters: 6754718720 \n",
      "# Trainable Parameter \n",
      "Before: 404280320 \n",
      "After: 6553600 \n",
      "Percentage: 1.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb7b4163ce744abac11715b2a0bd9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb18f88ea4e449acbe3a4d8ef71eb5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 模型配置 ###\n",
    "#model_id = \"./llama2/shakechen/Llama-2-7b-hf\"\n",
    "model_id = \"./xuanyuan/Duxiaoman-DI/XuanYuan-13B-Chat\"\n",
    "# max_length = 512\n",
    "# device_map = \"auto\"\n",
    "# batch_size = 128\n",
    "# micro_batch_size = 32\n",
    "max_length = 512\n",
    "device_map = \"auto\"\n",
    "batch_size = 8\n",
    "micro_batch_size = 2\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "# nf4\" use a symmetric quantization scheme with 4 bits precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# load model from huggingface\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# load tokenizer from huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    print(f\"trainable model parameters: {trainable_model_params}. All model parameters: {all_model_params} \")\n",
    "    return trainable_model_params\n",
    "    \n",
    "ori_p = print_number_of_trainable_model_parameters(model)\n",
    "\n",
    "# LoRA config\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "### compare trainable parameters #\n",
    "peft_p = print_number_of_trainable_model_parameters(model)\n",
    "print(f\"# Trainable Parameter \\nBefore: {ori_p} \\nAfter: {peft_p} \\nPercentage: {round(peft_p / ori_p * 100, 2)}\")\n",
    "\n",
    "#数据处理\n",
    "#dataset = datasets.load_dataset('json', data_files='data/databricks.jsonl', split='train')\n",
    "dataset = datasets.load_dataset('json', data_files='data/fine-tuning-data1.jsonl', split='train')\n",
    "\n",
    "### generate prompt based on template ###\n",
    "prompt_template = {\n",
    "    \"prompt_input\": \\\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides further context.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "    \n",
    "    \"prompt_no_input\": \\\n",
    "    \"Below is an instruction that describes a task.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "    \"response_split\": \"### Response:\"\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, input=None, label=None, prompt_template=prompt_template):\n",
    "    if input:\n",
    "        res = prompt_template[\"prompt_input\"].format(\n",
    "            instruction=instruction, input=input)\n",
    "    else:\n",
    "        res = prompt_template[\"prompt_no_input\"].format(\n",
    "            instruction=instruction)\n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    return res\n",
    "\n",
    "def tokenize(tokenizer, prompt, max_length=max_length, add_eos_token=False):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None)\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"context\"],\n",
    "        data_point[\"response\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(tokenizer, full_prompt)\n",
    "    user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"context\"])\n",
    "    tokenized_user_prompt = tokenize(tokenizer, user_prompt)\n",
    "    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "    mask_token = [-100] * user_prompt_len\n",
    "    tokenized_full_prompt[\"labels\"] = mask_token + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
    "    return tokenized_full_prompt\n",
    "    \n",
    "dataset = dataset.train_test_split(test_size=16, shuffle=True, seed=42)\n",
    "cols = [\"instruction\", \"context\", \"response\", \"category\"]\n",
    "train_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols)\n",
    "val_data = dataset[\"test\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b8b17c-b3dd-4fcf-b4af-4e35933e9c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     49\u001b[0m     model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxuanyuan-13B-int4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#model.save_pretrained(\"llama-7b-int4-dolly\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2732\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2730\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2731\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:1999\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1999\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2001\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "#模型训练步骤\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./llama-7b-int4-dolly\",\n",
    "#     #output_dir=\"./xuanyuan-13B-int4\",\n",
    "#     num_train_epochs=20,\n",
    "#     max_steps=200,\n",
    "#     fp16=True,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "#     learning_rate=2e-4,\n",
    "#     lr_scheduler_type=\"constant\",\n",
    "#     per_device_train_batch_size=micro_batch_size,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     gradient_checkpointing=True,\n",
    "#     group_by_length=False,\n",
    "#     logging_steps=10,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     save_total_limit=3,\n",
    "#     disable_tqdm=False,\n",
    "# )\n",
    "args = TrainingArguments(\n",
    "    #output_dir=\"./llama-7b-int4-dolly\",\n",
    "    output_dir=\"./xuanyuan-13B-int4\",\n",
    "    num_train_epochs=10,\n",
    "    max_steps=30,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "      tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n",
    ")\n",
    "\n",
    "# silence the warnings. re-enable for inference!\n",
    "with torch.no_grad():\n",
    "    model.config.use_cache = False\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"xuanyuan-13B-int4\")\n",
    "    #model.save_pretrained(\"llama-7b-int4-dolly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6639b2a1-439b-460c-9efb-b1c8ab2a1127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a44b05e00a48809c7c90265b18c824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:  <s> 什么是币币交易？OKX\n"
     ]
    }
   ],
   "source": [
    "#模型测试，就可以和没训练的模型进行比较\n",
    "# model path and weight\n",
    "model_id = \"./llama2/shakechen/Llama-2-7b-hf\"\n",
    "peft_path = \"./llama-7b-int4-dolly\"\n",
    "#model_id = \"./xuanyuan/Duxiaoman-DI/XuanYuan-13B-Chat\"\n",
    "#peft_path = \"./xuanyuan-13B-int4\"\n",
    "\n",
    "# loading model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# loading peft weight\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    peft_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# generation config\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4, # beam search\n",
    ")\n",
    "\n",
    "#generating reply\n",
    "with torch.no_grad():\n",
    "    prompt = \"什么是币币交易？\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        #max_new_tokens=320,\n",
    "    )\n",
    "    print('\\nAnswer: ', tokenizer.decode(generation_output.sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd322e8-9653-4cc3-a198-54600e120835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5b236b853347d19c7f943106a35558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "回答：在区块链领域，\"币币交易\"指的是一种特殊类型的数字货币兑换方式。这个过程涉及到两种或多种不同的加密货币之间进行直接转移和互相兑换。与传统金融市场中使用法定货币（如美元、欧元等）作为媒介来买卖其他资产形成鲜明对比的是，“币币”交易完全绕开了任何实体货币参与，而仅依赖于各自独立存在且具有价值属性的虚拟代码—即所谓的 \"去中心化\" 数字货币本身就可以被视为支付手段并执行购物消费功能。因此, 币币交易也常称为 “P2P 交易”, 即点对点(Peer-to-peer) 的电子现金系统模式下发生的事件。\n"
     ]
    }
   ],
   "source": [
    "#没训练的原始模型\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "#model_name_or_path = \"./llama2/shakechen/Llama-2-7b-hf\"\n",
    "model_name_or_path = \"./xuanyuan/Duxiaoman-DI/XuanYuan-13B-Chat\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path, use_fast=False, legacy=True)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name_or_path,torch_dtype=torch.float16, device_map=\"auto\")\n",
    "inputs = tokenizer(\"问题：什么是币币交易？\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=1280, repetition_penalty=1.1)\n",
    "outputs = tokenizer.decode(outputs.cpu()[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
